# RAG知识库

## 目录

- [1  什么是RAG](#1--什么是RAG)
- [2  RAG工作流程](#2--RAG工作流程)
  - [2.1  文档收集和切割](#21--文档收集和切割)
  - [2.2  向量转换和存储](#22--向量转换和存储)
  - [2.3  文档过滤和检索](#23--文档过滤和检索)
  - [2.4  查询增强和关联](#24--查询增强和关联)
- [3  RAG相关技术](#3--RAG相关技术)
  - [3.1  Embedding](#31--Embedding)
  - [3.2  向量数据库](#32--向量数据库)
  - [3.3  召回](#33--召回)
  - [3.4  精排和Rank模型](#34--精排和Rank模型)
  - [3.5  混合检索策略](#35--混合检索策略)
- [4  大模型幻觉问题](#4--大模型幻觉问题)
  - [4.1  什么是大模型幻觉](#41--什么是大模型幻觉)
  - [4.2  为什么大模型会出现幻觉](#42--为什么大模型会出现幻觉)
  - [4.3  如何减少大模型幻觉](#43--如何减少大模型幻觉)
- [5  评估RAG系统](#5--评估RAG系统)
  - [5.1  RAG 应用的评估指标](#51--RAG-应用的评估指标)
  - [5.2  评估流程：从构建数据到系统优化](#52--评估流程从构建数据到系统优化)
- [6  GraphRAG](#6--GraphRAG)
  - [6.1  GraphRAG是什么](#61--GraphRAG是什么)
  - [6.2  运行GraphRAG](#62--运行GraphRAG)
  - [6.3  查询](#63--查询)
- [7  RAG 研究参考论文](#7--RAG-研究参考论文)

## 1  什么是RAG

**RAG（Retr﻿ieval-Augmented ‌Generation，检索增强生‎成）**  是一种结合信息检索技术和 A‍I 内容生成的混合架构，可以解决⁠大模型的知识时效性限制和幻觉问题。

简单来说，RA﻿G 就像给 AI 配了一个‌ “小抄本”，让 AI 回‎答问题前先查一查特定的知识‍库来获取知识，确保回答是基⁠于真实资料而不是凭空想象。

R﻿AG 在大语言模型生成回答之前‌，会先从外部知识库中检索相关信‎息，然后将这些检索到的内容作为‍额外上下文提供给模型，引导其生⁠成更准确、更相关的回答。该方法涉及批处理式编程模型，其中作业从文档中读取非结构化数据，进行转换，然后将其写入向量数据库。从高层次上讲，这是一个 ETL（提取、转换和加载）流程。向量数据库用于 RAG 技术的检索部分。

RAG 的下一个阶段是处理用户输入。当 AI 模型需要回答用户的问题时，该问题和所有“相似”的文档片段都会被放入发送给 AI 模型的提示中。这就是使用矢量数据库的原因。它非常擅长查找相似内容。

![](assets/image_2ghDyWEQBk-20250804134439-p0ypg66.png)

## 2  RAG工作流程

RAG 技﻿术实现主要包含以下‌ 4 个核心步骤：

- 文档收集和切割
- 向量转换和存储
- 文档过滤和检索
- 查询增强和关联

### 2.1  文档收集和切割

- **文档收集：**  从网页、PDF、数据库等来源获取原始资料。
- \*\*文档预处理： \*\*包括去除噪声、格式标准化、编码统一等。
- **文档切割（Chunking）：**  将长文档切分为便于建模的小段落，常见方法有：
  - 基于固定大小（如 512 个 token）
  - 基于语义边界（如段落、章节）
  - 基于递归分割策略（如递归字符 n-gram 切割）

文档收集和切割阶段，我们要对自己准备好的知识库文档进行处理，然后保存到向量数据库中。这个过程俗称 ETL（抽取、转换、加载），Spring AI 提供了对 ETL 的支持，参考 [官方文档](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html "官方文档")。

在 Spr﻿ing AI 中，‌对 Documen‎t 的处理通常遵循‍以下流程：

1. 读取文档：使用 DocumentReader 组件从数据源（如本地文件、网络资源、数据库等）加载文档。
2. 转换文档：根据需求将文档转换为适合后续处理的格式，比如去除冗余信息、分词、词性标注等，可以使用 DocumentTransformer 组件实现。
3. 写入文档：使用 DocumentWriter 将文档以特定格式保存到存储中，比如将文档以嵌入向量的形式写入到向量数据库，或者以键值对字符串的形式保存到 Redis 等 KV 存储中。

### 2.2  向量转换和存储

- **向量转换：** ﻿使用 Embedd‌ing 模型将文本‎块转换为高维向量表‍示，可以捕获到文本⁠的语义特征。
- **向量存储：** ﻿将生成的向量和对应‌文本存入向量数据库‎，支持高效的相似性‍搜索。

### 2.3  文档过滤和检索

- **查询向量化：**  将用户输入的问题通过相同的 Embedding 模型转化为查询向量。
- \*\*过滤机制： \*\*可依据文档的元数据（时间、标签、来源等）或关键字实现初步过滤。
- **相似度检索：**  在向量库中基于余弦相似度或欧几里得距离等算法检索最相关的文档块。
- **上下文组装：**  将多个相关文档片段组合为结构化上下文，供后续大模型使用。

Spring A﻿I 官方声称提供了一个 “模块‌化” 的 RAG 架构，用于优‎化大模型回复的准确性。简单来说，﻿就是把整个文档过滤‌检索阶段拆分为：检‎索前、检索时、检索‍后，分别针对每个阶⁠段提供了可自定义的组件。

1. 在预检索阶段，系统接收用户的原始查询，通过查询转换和查询扩展等方法对其进行优化，输出增强的用户查询。
2. 在检索阶段，系统使用增强的查询从知识库中搜索相关文档，可能涉及多个检索源的合并，最终输出一组相关文档。
3. 在检索后阶段，系统对检索到的文档进行进一步处理，包括排序、选择最相关的子集以及压缩文档内容，输出经过优化的相关文档集。

### 2.4  查询增强和关联

- **提示词构造（Prompt Assembly）：**  将用户问题与检索到的文档内容拼接形成增强提示（Augmented Prompt）。
- \*\*上下文融合与生成： \*\*大模型基于增强提示进行答案生成，结合上下文提升准确性和相关性。
- \*\*引用与可追溯性： \*\*在回答中附加引用来源，增强结果可信度。
- \*\*答案后处理： \*\*对生成结果进行格式化、摘要压缩等优化，以提升可读性和用户体验。

Spring AI 提供的 2 种实现 RAG 查询增强的 Advisor，分别是 QuestionAnswerAdvisor 和 RetrievalAugmentationAdvisor。

## 3  RAG相关技术

### 3.1  Embedding

Embeddin﻿g 嵌入是将高维离散数据（如文‌字、图片）转换为低维连续向量的‎过程。这些向量能在数学空间中表‍示原始数据的语义特征，使计算机⁠能够理解数据间的相似性。

Embedding 模型是﻿执行这种转换算法的机器学习模型，常用的模型有：

- OpenAI's text-embedding-3-small
- BERT / SBERT（Sentence-BERT）
- GTE（General Text Embedding，智谱AI）
- Cohere Embed、多语言 MiniLM 等

不同的 Emb‎edding 模型产生的向量表示和维度数不同，一般‍维度越高表达能力更强，可以捕获更丰富的语义信息和更⁠细微的差别，但同样占用更多存储空间。

相似含义的文本在向量空间中靠近。例如，“﻿鸡” 和 “鸡肉” 的‌ Embedding 向‎量在空间中较接近，而 “‍鸡” 和 “篮球” 则⁠相距较远，反映了语义关系。

### 3.2  向量数据库

向量数据库﻿是专门存储和检索向量‌数据的数据库系统。通‎过高效索引算法实现快‍速相似性搜索，支持 ⁠K 近邻查询等操作。

**功能特性：**

- 支持向量检索（Top-K 相似度搜索）
- 支持元数据过滤（如按标签、日期等）
- 支持增量更新、删除、并发查询

**检索方式：**

- 余弦相似度（Cosine Similarity）
- 欧氏距离（Euclidean Distance）
- 点积（Inner Product）

注意，并不﻿是只有向量数据库才‌能存储向量数据，只‎不过与传统数据库不‍同，向量数据库优化⁠了高维向量的存储和检索。

### 3.3  召回

召回是从大规模文档库中初步筛选出与查询最相关的一批文档候选，通常基于向量相似度。**强调速度和广度，而非精确度。**

### 3.4  精排和Rank模型

精排（精确排﻿序）是搜索 / 推荐系统‌的最后阶段，使用计算复杂‎度更高的算法，考虑更多特‍征和业务规则，对少量候选⁠项进行更复杂、精细的排序。

精排是在召回候选集基础上，通过 Rank 模型对候选文档重新评分和排序，选出最相关的文档输入到生成模型。

### 3.5  混合检索策略

混合检索策﻿略结合多种检索方法‌的优势，提高搜索‎效果。常见组合包括关‍键词检索、语义检索、知⁠识图谱等。比如在 AI 大﻿模型开发平台 Dify 中，就为‌用户提供了 “基于全文检索的关键‎词搜索 + 基于向量检索的语义检‍索” 的混合检索策略，用户还可以⁠自己设置不同检索方式的权重。

## 4  大模型幻觉问题

随着大语言模型（LLMs）的广泛应用，我们时常会遇到一个问题：**模型明明说得头头是道，结果却是错的**。这类问题被称为“大模型幻觉”（AI Hallucination）。

### 4.1  什么是大模型幻觉

大模型幻觉，指的是模型生成了**看似合理却不准确甚至虚构的内容**。就像一个自信满满的学生，在不理解问题的前提下给出了一个“自圆其说但错误”的回答。

**常见的三种幻觉类型：**

1. **事实性幻觉**： &#x20;

    模型生成与真实世界事实不符的内容。

    - 示例：“坤坤发明了计算器。”
2. **逻辑性幻觉**： &#x20;

    推理过程有问题，得出错误结论。

    - 示例：“1 + 1 = 3。”
3. **自洽性幻觉**： &#x20;

    生成内容自身存在矛盾。

    - 示例：“我很年轻，才 80 岁。”

### 4.2  为什么大模型会出现幻觉

这个问题的根源主要有三点：

- **训练数据的问题**：模型从互联网数据中学习，这些数据中本身可能包含错误、过时甚至虚构的信息。
- **生成机制的限制**：LLMs 的核心是**预测下一个词的概率模型**，它们擅长生成流畅的文本，但不一定注重事实准确。
- 大模型并不具备真正的认知能力，它只是学会了人类语言的统计模式。想象你问一个从没去过月球的人关于月球的情况，他可能会根据电影或小说给出似是而非的答案。

### 4.3  如何减少大模型幻觉

1\. 引入 RAG（检索增强生成）

    RAG 是目前最有效的减幻方法之一。通过引入**外部知识库**，大模型不再依赖记忆中的信息，而是“现查现用”。

    **引用标注机制**：RAG 系统可以在回答中标注信息来源，提升可信度。

    **不确定时保持诚实**：比起“猜测”，鼓励模型在无法确定时表达“不知道”。

2\. 提示工程优化（Prompt Engineering）

使用“**思维链（Chain-of-Thought）** ”等策略，引导模型**逐步推理**，不仅提升推理质量，还让我们更容易看出错误在哪一步出现。

示例：很多 AI Agent（智能体）采用这种方式来增强透明度与控制力。

3\. **事实验证模型**

建立关键信息的自动核查机制，或实施人机协作的审核流程。评估幻觉程度的指标包括事实一致性、引用准确性和自洽性评分。

## 5  评估RAG系统

> **评估的核心在于回答三个关键问题：** 
> 1\. 检索的内容是否与问题高度相关？
> 2\. 生成的回答是否准确、有价值？
> 3\. 用户使用时的整体体验是否良好？

### 5.1  RAG 应用的评估指标

1. 检索质量评估指标
    - 召回率（Recall）：是否能覆盖所有相关文档？
    - 精确率（Precision）：检索结果中有多大比例是真正相关的？
    - 平均精度均值（MAP）：在考虑排序的情况下，整体精度表现如何？
    - 规范化折扣累积增益（NDCG）：文档越相关且排得越靠前，得分越高。
2. 生成回答质量评估指标
    - 事实准确性：回答中事实性陈述的准确程度
    - 答案完整性：回答是否涵盖问题的所有方面
    - 上下文相关性：回答与问题的相关程度
    - 引用准确性：引用内容是否确实来自检索上下文

### 5.2  评估流程：从构建数据到系统优化

1. 构建评估数据集

    设计覆盖多类型问题（如事实型、推理型、观点型）的标准测试集。每个测试样本包括：

    - 问题描述
    - 标准答案
    - 相关文档 Ground Truth
      确保测试集具有**多样性与代表性**，是评估效果的前提。
2. 执行检索评估

    对每条测试问题运行实际检索过程，并将系统返回的文档与人工标注进行对比，计算召回率、NDCG 等指标。

    这一步可以很好地发现**检索策略是否合理**，是否需要调整 Embedding、索引、过滤规则等模块。
3. 评估生成回答质量

    采用**自动评估 + 人工评估** 相结合的方式：

    - 自动评估指标如：
      - **BLEU**（常用于多语言翻译）
      - **ROUGE**（用于摘要评估）
      - **BERTScore**（语义相似性）
    - 人工评估则关注：
      - 创造性
      - 可读性
      - 实用性
      - 自洽性
4. 分析与优化

    综合检索与生成两个阶段的表现，**识别失败模式**，是调优的关键。

    常见的失败类型：

    - **检索不到关键文档** → 检索策略或向量质量问题
    - **检索内容正确但回答错误** → 提示词构建不合理或模型能力不足
    - **生成内容偏题、啰嗦、自相矛盾** → 上下文组织、prompt 或输出后处理需优化

## 6  GraphRAG

### 6.1  GraphRAG是什么

LLM 由于不可避免的幻觉问题，在部分场景的应用效果不佳，借助 RAG 方案可以大幅提升 LLM 的生成质量与可用性。GraphRAG 是由微软研究院开发，它通过结合大型语言模型（LLM）和知识图谱，显著提高了AI在处理复杂信息和大型数据集上的问答和主题发现能力。相比基线 RAG，GraphRAG 在全面理解大型数据集方面具有更好的性能。 &#x20;

GraphRAG的工作原理包括以下几个步骤：

1. 提取知识图谱：首先，GraphRAG从原始文本中创建一个“知识图谱”。这个知识图谱就像一个连接想法的网络，每个想法（或“节点”）都以有意义的方式与其他节点相连。
2. 建立社区层次结构：接下来，GraphRAG将这些连接的想法组织成组，或称为“社区”。这些社区可以视为相关概念的集群。
3. 生成摘要：对于每个社区，GraphRAG会生成摘要，捕捉主要点。这有助于理解关键思想，而不会在细节中迷失。
4. 利用结构：当你需要执行涉及检索和生成信息的任务（基于RAG的任务）时，GraphRAG使用这个组织良好的结构。这使得过程更加高效和准确。 &#x20;

    GraphRAG的优势在于它能够提供更准确、上下文相关且全面的答案，相比于传统的仅基于向量的RAG方法。它增强了AI对复杂和私有数据的推理能力，通过以更智能的方式组织信息，允许AI做出更好的决策并提供更多准确的响应。

### 6.2  运行GraphRAG

运行 GraphRAG 只需`pip install graphrag`即可，具体使用方法可参考官方手册：[Getting Started](https://microsoft.github.io/graphrag/get_started/ "Getting Started")。
使用 GraphRAG 前需要先初始化项目，在项目路径`./graphtest`运行

```python
python -m graphrag.index --init --root ./graphtest
```

此时在项目路径下会有：

- Input：用于存放项目知识
- Output：用于运行文件
- prompts：提示词
- .env：api key
- settings.yaml：配置LLM &#x20;

  要使用 GLM 系列模型运行 GraphRAG 需要分别配置 `.env`以及`settings.yaml`两个文件。 &#x20;

  在`.env`中，配置你的 api key

  ```text
  GRAPHRAG_API_KEY=<you api key>
  ```

在`settings.yaml`中配置LLM：

- 将`model`修改为智谱BigModel大模型，并将`api_base`配置为智谱大模型的请求URL，例如：
  ```yaml
  llm:
    api_key: ${GRAPHRAG_API_KEY}
    type: openai_chat
    model: glm-4-air    # 修改LLM
    api_base: https://open.bigmodel.cn/api/paas/v4    # 修改请求URL
  embeddings:
    async_mode: threaded
    llm:
      api_key: ${GRAPHRAG_API_KEY}
      type: openai_embedding
      model: embedding-3    # 修改向量模型
      api_base: https://open.bigmodel.cn/api/paas/v4    # 修改请求URL
  ```

在 Input 目录中存入文本资料后，运行 GraphRAG 构建知识图谱。构建知识图谱的过程需要根据文本数据量的大小等待一定时长，首次运行建议使用较小的文本进行测试。

```python
python -m graphrag.index --root ./graphtest
```

### 6.3  查询

GraphRAG的查询模式分为全局查询和局部查询：

- 全局查询：利用知识图谱的层级摘要对整个知识库进行推理总结，适合进行全局的总结分析和摘要以及创造；

  ```python
  python -m graphrag.query --root ./graphtest --method global "your query" 
  ```
- 局部查询：通过扩展相关实体和概念来对特定实体进行推理，适合对特定问题进行分析和总结。

  ```python
  python -m graphrag.query --root ./graphtest --method local "your query"
  ```

## 7  RAG 研究参考论文

这里列出了一些重要的研究论文，它们揭示了 RAG 领域的关键洞察和最新进展。

|**洞见**​|**参考来源**​|**发布日期**​|
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| ----------------------|
|提出一种名为纠正检索增强生成（CRAG, Corrective Retrieval Augmented Generation）的方法，旨在提升 RAG 系统生成内容的稳定性和准确性。其核心在于增加一个能够自我修正的组件至检索器中，并优化检索文档的使用，以促进更优质的内容生成。此外，引入了一种检索评估机制，用于评价针对特定查询检索到的文档的整体品质。通过网络搜索和知识的优化利用，能够有效提升文档自我修正和利用的效率。|\[纠正检索增强生成(opens in a new tab)]\([https://arxiv.org/abs/2401.15884](https://arxiv.org/abs/2401.15884) "纠正检索增强生成(opens in a new tab)")|2024年1月|
|RAPTOR 模型通过递归方式嵌入、聚类并总结文本信息，自底向上构建出层次化的总结树。在使用时，该模型能够从这棵树中检索信息，实现对长文档在不同抽象层面上信息的综合利用。|\[RAPTOR：递归抽象处理用于树组织检索(opens in a new tab)]\([https://arxiv.org/abs/2401.18059](https://arxiv.org/abs/2401.18059) "RAPTOR：递归抽象处理用于树组织检索(opens in a new tab)")|2024年1月|
|开发了一个通用框架，通过大语言模型（LLM）与检索器之间的多步骤互动，有效处理多标签分类难题。|\[在上下文中学习用于极端多标签分类(opens in a new tab)]\([https://arxiv.org/abs/2401.12178](https://arxiv.org/abs/2401.12178) "在上下文中学习用于极端多标签分类(opens in a new tab)")|2024年1月|
|研究表明，通过提取高资源语言中语义相似的提示，可以显著提升多语言预训练语言模型在多种任务上的零样本学习能力。|\[从分类到生成：洞察跨语言检索增强的 ICL(opens in a new tab)]\([https://arxiv.org/abs/2311.06595](https://arxiv.org/abs/2311.06595) "从分类到生成：洞察跨语言检索增强的 ICL(opens in a new tab)")|2023年11月|
|针对 RAGs 模型在处理噪声较多、不相关文档以及未知情境时的稳健性进行了改善，通过为检索文档生成序列化阅读笔记，深入评估其与提问的相关性，并整合信息以构建最终答案。|\[链式笔记：增强检索增强语言模型的鲁棒性(opens in a new tab)]\([https://arxiv.org/abs/2311.09210](https://arxiv.org/abs/2311.09210) "链式笔记：增强检索增强语言模型的鲁棒性(opens in a new tab)")|2023年11月|
|通过去除可能不会对答案生成贡献关键信息的标记，优化了检索增强阅读模型的处理流程，实现了高达 62.2% 的运行时间缩减，同时保持性能仅降低了2%。|\[通过标记消除优化检索增强阅读器模型(opens in a new tab)]\([https://arxiv.org/abs/2310.13682](https://arxiv.org/abs/2310.13682) "通过标记消除优化检索增强阅读器模型(opens in a new tab)")||
|通过对小型语言模型 (LM) 进行指令式微调，我们开发了一个独立的验证器，以验证知识增强语言模型 (knowledge-augmented LMs) 的输出及其知识准确性。这种方法特别有助于解决模型在面对特定查询时未能检索相关知识，或在生成文本中未能准确反映检索到的知识的情况。|\[知识增强语言模型验证(opens in a new tab)]\([https://arxiv.org/abs/2310.12836](https://arxiv.org/abs/2310.12836) "知识增强语言模型验证(opens in a new tab)")|2023年10月|
|我们设立了一个基准测试，以分析不同大型语言模型 (LLMs) 在检索增强生成 (RAG) 所需的四项核心能力——噪声容忍、排除不相关信息、信息融合和对反事实情境的适应性——的表现。|\[大型语言模型在检索增强生成中的基准测试(opens in a new tab)]\([https://arxiv.org/abs/2309.01431](https://arxiv.org/abs/2309.01431) "大型语言模型在检索增强生成中的基准测试(opens in a new tab)")|2023年10月|
|介绍了一种自我反思的检索增强生成 (Self-RAG) 框架，旨在通过检索和自我反思来提升语言模型的质量和事实性。该框架利用语言模型动态检索信息，并通过反思标记来生成和评估检索到的内容及其自生成内容。|\[自我反思检索增强生成: 通过自我反思学习检索、生成及自我批判(opens in a new tab)]\([https://arxiv.org/abs/2310.11511](https://arxiv.org/abs/2310.11511) "自我反思检索增强生成: 通过自我反思学习检索、生成及自我批判(opens in a new tab)")|2023年10月|
|通过生成增强检索 (GAR) 和检索增强生成 (RAG) 的迭代改善，提高了零样本信息检索的能力。该过程中的改写-检索阶段有效提升了召回率，而重排阶段则显著提高了精度。|\[零样本信息检索中的GAR与RAG相结合的新范式(opens in a new tab)]\([https://arxiv.org/abs/2310.20158](https://arxiv.org/abs/2310.20158) "零样本信息检索中的GAR与RAG相结合的新范式(opens in a new tab)")|2023年10月|
|通过使用基于 43B GPT 模型的预训练和从 1.2 万亿 Token 中检索信息，我们预训练了一个 48B 的检索模型。进一步通过指令式微调，该模型在多种零样本任务上相比经过指令式微调的 GPT 模型显示出显著的性能提升。|\[InstructRetro: 检索增强预训练后的指令式微调(opens in a new tab)]\([https://arxiv.org/abs/2310.07713](https://arxiv.org/abs/2310.07713) "InstructRetro: 检索增强预训练后的指令式微调(opens in a new tab)")|2023年10月|
|通过两步精细调整，我们为大型语言模型增加了检索功能：一步是优化预训练的语言模型以更有效利用检索到的信息，另一步则是改进检索器以返回更符合语言模型偏好的相关结果。这种分阶段的微调方法，在要求知识利用和上下文感知的任务中，显著提升了性能。|\[检索增强的双重指令微调 (RA-DIT)(opens in a new tab)]\([https://arxiv.org/abs/2310.01352](https://arxiv.org/abs/2310.01352) "检索增强的双重指令微调 (RA-DIT)(opens in a new tab)")|2023年10月|
|介绍了一种提升 RAGs 在面对不相关内容时鲁棒性的方法。该方法通过在训练期间混合使用相关与不相关的上下文，自动产生数据以微调语言模型，从而有效利用检索到的文段。|\[让基于检索增强的语言模型对无关上下文更加鲁棒(opens in a new tab)]\([https://arxiv.org/abs/2310.01558](https://arxiv.org/abs/2310.01558) "让基于检索增强的语言模型对无关上下文更加鲁棒(opens in a new tab)")|2023年10月|
|研究表明，采用简单检索增强技术的 4K 上下文窗口的大语言模型在生成过程中，其表现与通过位置插值对长上下文任务进行微调的 16K 上下文窗口的大语言模型相媲美。|\[当检索遇上长上下文的大语言模型(opens in a new tab)]\([https://arxiv.org/abs/2310.03025](https://arxiv.org/abs/2310.03025) "当检索遇上长上下文的大语言模型(opens in a new tab)")|2023年10月|
|在上下文融合前将检索文档压缩为文本摘要，既降低了计算成本，也减轻了模型从长文档中识别关键信息的难度。|\[RECOMP: 用压缩和选择性增强提升检索增强语言模型(opens in a new tab)]\([https://arxiv.org/abs/2310.04408](https://arxiv.org/abs/2310.04408) "RECOMP: 用压缩和选择性增强提升检索增强语言模型(opens in a new tab)")|2023年10月|
|提出了一个迭代式的检索与生成协同工作框架，它结合了参数化和非参数化知识，通过检索与生成的互动来寻找正确的推理路径。这一框架特别适合需要多步推理的任务，能够显著提高大语言模型的推理能力。|\[检索与生成的协同作用加强了大语言模型的推理能力(opens in a new tab)]\([https://arxiv.org/abs/2310.05149](https://arxiv.org/abs/2310.05149) "检索与生成的协同作用加强了大语言模型的推理能力(opens in a new tab)")|2023年10月|
|提出“澄清树”框架，该框架通过少样本提示并借助外部知识，为含糊问题递归构建一个消歧树。然后利用这棵树产生详细的答案。|\[利用检索增强大语言模型回答含糊问题的“澄清树”方法(opens in a new tab)]\([https://arxiv.org/abs/2310.14696](https://arxiv.org/abs/2310.14696) "利用检索增强大语言模型回答含糊问题的“澄清树”方法(opens in a new tab)")|2023年10月|
|介绍了一种使大语言模型能够参考其之前遇到的问题，并在面对新问题时动态调用外部资源的方法。|\[借助自我知识的大语言模型检索增强策略(opens in a new tab)]\([https://arxiv.org/abs/2310.05002](https://arxiv.org/abs/2310.05002) "借助自我知识的大语言模型检索增强策略(opens in a new tab)")|2023年10月|
|提供了一组评估指标，用于从多个维度（如检索系统识别相关及集中上下文段落的能力、大语言模型忠实利用这些段落的能力，以及生成内容本身的质量）评价不同方面，而无需依赖人工注释的真实数据。|\[RAGAS: 对检索增强生成进行自动化评估的指标体系(opens in a new tab)]\([https://arxiv.org/abs/2309.15217](https://arxiv.org/abs/2309.15217) "RAGAS: 对检索增强生成进行自动化评估的指标体系(opens in a new tab)")|2023年9月|
|提出了一种创新方法——生成后阅读（GenRead），它让大型语言模型先根据提问生成相关文档，再从这些文档中提取答案。|\[生成而非检索：大型语言模型作为强大的上下文生成器(opens in a new tab)]\([https://arxiv.org/abs/2209.10063](https://arxiv.org/abs/2209.10063) "生成而非检索：大型语言模型作为强大的上下文生成器(opens in a new tab)")|2023年9月|
|展示了在 RAG 系统中如何使用特定排名器（比如 DiversityRanker 和 LostInTheMiddleRanker）来挑选信息，从而更好地利用大型语言模型的上下文窗口。|\[提升 Haystack 中 RAG 系统的能力：DiversityRanker 和 LostInTheMiddleRanker 的引入(opens in a new tab)]\([https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5](https://towardsdatascience.com/enhancing-rag-pipelines-in-haystack-45f14e2bc9f5) "提升 Haystack 中 RAG 系统的能力：DiversityRanker 和 LostInTheMiddleRanker 的引入(opens in a new tab)")|2023年8月|
|描述了如何将大型语言模型与不同的知识库结合，以便于知识的检索和储存。通过编程思维的提示来生成知识库的搜索代码，此外，还能够根据用户的需要，将知识储存在个性化的知识库中。|\[KnowledGPT: 利用知识库检索和存储功能增强大型语言模型(opens in a new tab)]\([https://arxiv.org/abs/2308.11761](https://arxiv.org/abs/2308.11761) "KnowledGPT: 利用知识库检索和存储功能增强大型语言模型(opens in a new tab)")|2023年8月|
|提出一种模型，通过结合检索增强掩码语言建模和前缀语言建模，引入上下文融合学习，以此提高少样本学习的效果，使模型能够在不增加训练负担的情况下使用更多上下文示例。|\[RAVEN: 借助检索增强编解码器语言模型实现的上下文学习(opens in a new tab)]\([https://arxiv.org/abs/2308.07922](https://arxiv.org/abs/2308.07922) "RAVEN: 借助检索增强编解码器语言模型实现的上下文学习(opens in a new tab)")|2023年8月|
|RaLLe 是一款开源工具，专门用于开发、评估和提升针对知识密集型任务的 RAG 系统的性能。|\[RaLLe: 针对检索增强大型语言模型的开发和评估框架(opens in a new tab)]\([https://arxiv.org/abs/2308.10633](https://arxiv.org/abs/2308.10633) "RaLLe: 针对检索增强大型语言模型的开发和评估框架(opens in a new tab)")|2023年8月|
|研究发现，当相关信息的位置发生变化时，大型语言模型的性能会明显受影响，这揭示了大型语言模型在处理长篇上下文信息时的局限性。|\[中途迷失：大型语言模型处理长篇上下文的方式(opens in a new tab)]\([https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172) "中途迷失：大型语言模型处理长篇上下文的方式(opens in a new tab)")|2023年7月|
|通过迭代的方式，模型能够将检索和生成过程相互协同。模型的输出不仅展示了完成任务所需的内容，还为检索更多相关知识提供了丰富的上下文，从而在下一轮迭代中帮助产生更优的结果。|\[通过迭代检索-生成协同增强检索增强的大语言模型(opens in a new tab)]\([https://arxiv.org/abs/2305.15294](https://arxiv.org/abs/2305.15294) "通过迭代检索-生成协同增强检索增强的大语言模型(opens in a new tab)")|2023年5月|
|介绍了一种新的视角，即在文本生成过程中，系统能够主动决定何时以及检索什么信息。接着，提出了一种名为FLARE的方法，通过预测下一句话来预见未来的内容，利用此内容作为关键词检索相关文档，并在发现不确定的表达时重新生成句子。|\[主动检索增强生成(opens in a new tab)]\([https://arxiv.org/abs/2305.06983](https://arxiv.org/abs/2305.06983) "主动检索增强生成(opens in a new tab)")|2023年5月|
|提出了一个能够通用应用于各种大语言模型的检索插件，即使在模型未知或不能共同微调的情况下也能提升模型性能。|\[适应增强型检索器改善大语言模型的泛化作为通用插件(opens in a new tab)]\([https://arxiv.org/abs/2305.17331](https://arxiv.org/abs/2305.17331) "适应增强型检索器改善大语言模型的泛化作为通用插件(opens in a new tab)")|2023年5月|
|通过两种创新的预训练方法，提高了对结构化数据的密集检索效果。首先，通过对结构化数据和非结构化数据之间的关联进行预训练来提升模型的结构感知能力；其次，通过实现遮蔽实体预测来更好地捕捉结构语义。|\[结构感知的语言模型预训练改善结构化数据上的密集检索(opens in a new tab)]\([https://arxiv.org/abs/2305.19912](https://arxiv.org/abs/2305.19912) "结构感知的语言模型预训练改善结构化数据上的密集检索(opens in a new tab)")|2023年5月|
|该框架能够动态地融合来自不同领域的多样化信息源，以提高大语言模型的事实准确性。通过一个自适应的查询生成器，根据不同知识源定制查询，确保信息的准确性逐步得到修正，避免错误信息的累积和传播。|\[知识链：通过动态知识适应异质来源来基础大语言模型(opens in a new tab)]\([https://arxiv.org/abs/2305.13269](https://arxiv.org/abs/2305.13269) "知识链：通过动态知识适应异质来源来基础大语言模型(opens in a new tab)")|2023年5月|
|此框架通过首先检索知识图谱中的相关子图，并通过调整检索到的子图的词嵌入来确保事实的一致性，然后利用对比学习确保生成的对话与知识图谱高度一致，为生成与上下文相关且基于知识的对话提供了新方法。|\[用于知识基础对话生成的知识图谱增强大语言模型(opens in a new tab)]\([https://arxiv.org/abs/2305.18846](https://arxiv.org/abs/2305.18846) "用于知识基础对话生成的知识图谱增强大语言模型(opens in a new tab)")|2023年5月|
|通过采用小型语言模型作为可训练重写器，以适应黑盒式大语言模型（LLM）的需求。重写器通过强化学习（RL）根据 LLM 的反馈进行训练，从而构建了一个名为“重写-检索-阅读”的新框架，专注于查询优化。|\[为检索增强的大语言模型重写查询(opens in a new tab)]\([https://arxiv.org/abs/2305.14283](https://arxiv.org/abs/2305.14283) "为检索增强的大语言模型重写查询(opens in a new tab)")|2023年5月|
|利用检索增强生成器迭代创建无限记忆池，并通过记忆选择器挑选出适合下一轮生成的记忆。此方法允许模型利用自身产出的记忆，称为“自我记忆”，以提升内容生成质量。|\[自我提升：带有自我记忆的检索增强文本生成(opens in a new tab)]\([https://arxiv.org/abs/2305.02437](https://arxiv.org/abs/2305.02437) "自我提升：带有自我记忆的检索增强文本生成(opens in a new tab)")|2023年5月|
|通过为大语言模型（LLM）装配知识引导模块，让它们在不改变内部参数的情况下，获取相关知识。这一策略显著提高了模型在需要丰富知识的领域任务（如事实知识增加7.9%，表格知识增加11.9%，医学知识增加3.0%，多模态知识增加8.1%）的表现。|\[用参数知识引导增强大语言模型(opens in a new tab)]\([https://arxiv.org/abs/2305.04757](https://arxiv.org/abs/2305.04757) "用参数知识引导增强大语言模型(opens in a new tab)")|2023年5月|
|为大语言模型（LLM）引入了一个通用的读写记忆单元，允许它们根据任务需要从文本中提取、存储并回忆知识。|\[RET-LLM：朝向大语言模型的通用读写记忆(opens in a new tab)]\([https://arxiv.org/abs/2305.14322](https://arxiv.org/abs/2305.14322) "RET-LLM：朝向大语言模型的通用读写记忆(opens in a new tab)")|2023年5月|
|通过使用任务不可知检索器，构建了一个共享静态索引，有效选出候选证据。随后，设计了一个基于提示的重排机制，根据任务的特定相关性重新排序最相关的证据，为读者提供精准信息。|\[针对非知识密集型任务的提示引导检索增强(opens in a new tab)]\([https://arxiv.org/abs/2305.17653](https://arxiv.org/abs/2305.17653) "针对非知识密集型任务的提示引导检索增强(opens in a new tab)")|2023年5月|
|提出了UPRISE（通用提示检索以改善零样本评估），通过调整一个轻量级且多功能的检索器，它能自动为给定零样本任务的输入检索出最合适的提示，以此来改善评估效果。|\[UPRISE：改进零样本评估的通用提示检索(opens in a new tab)]\([https://arxiv.org/abs/2303.08518](https://arxiv.org/abs/2303.08518) "UPRISE：改进零样本评估的通用提示检索(opens in a new tab)")|2023年3月|
|结合了 SLMs 作为过滤器和 LLMs 作为重排器的优势，提出了一个适应性的“过滤-再重排”范式，有效提升了难样本的信息提取与重排效果。|\[大语言模型不是一个理想的少样本信息提取器，但它在重排难样本方面表现出色！(opens in a new tab)]\([https://arxiv.org/abs/2303.08559](https://arxiv.org/abs/2303.08559) "大语言模型不是一个理想的少样本信息提取器，但它在重排难样本方面表现出色！(opens in a new tab)")|2023年3月|
|零样本学习指导一款能够遵循指令的大语言模型，创建一个虚拟文档来抓住重要的联系模式。接着，一个名为Contriever的工具会将这份文档转化成嵌入向量，利用这个向量在大数据集的嵌入空间中找到相似文档的聚集地，通过向量的相似度来检索真实文档。|\[无需相关标签的精确零样本密集检索(opens in a new tab)]\([https://arxiv.org/abs/2212.10496](https://arxiv.org/abs/2212.10496) "无需相关标签的精确零样本密集检索(opens in a new tab)")|2022年12月|
|提出了一个名为展示-搜索-预测（DSP）的新框架，通过这个框架可以编写高级程序，这些程序能够先展示流程，然后搜索相关信息，并基于这些信息做出预测。它能够将复杂问题分解成小的、更易于解决的步骤。|\[通过检索和语言模型组合，为复杂的自然语言处理任务提供解决方案(opens in a new tab)]\([https://arxiv.org/abs/2212.14024](https://arxiv.org/abs/2212.14024) "通过检索和语言模型组合，为复杂的自然语言处理任务提供解决方案(opens in a new tab)")|2022年12月|
|采用了一种新的多步骤问答策略，通过在思维链条的每一步中穿插检索信息，使用检索到的信息来丰富和改善思维链条。这种方法显著提升了解决知识密集型多步问题的效果。|\[结合思维链条推理和信息检索解决复杂多步骤问题(opens in a new tab)]\([https://arxiv.org/abs/2212.10509](https://arxiv.org/abs/2212.10509) "结合思维链条推理和信息检索解决复杂多步骤问题(opens in a new tab)")|2022年12月|
|研究发现，增加检索环节可以有效减轻对已有训练信息的依赖，使得RAG变成一个有效捕捉信息长尾的策略。|\[大语言模型在学习长尾知识方面的挑战(opens in a new tab)]\([https://arxiv.org/abs/2211.08411](https://arxiv.org/abs/2211.08411) "大语言模型在学习长尾知识方面的挑战(opens in a new tab)")|2022年11月|
|通过抽样方式，从大语言模型的记忆中提取相关信息段落，进而生成最终答案。|\[通过回忆增强语言模型的能力(opens in a new tab)]\([https://arxiv.org/abs/2210.01296](https://arxiv.org/abs/2210.01296) "通过回忆增强语言模型的能力(opens in a new tab)")|2022年10月|
|将大语言模型用作少量示例的查询生成器，根据这些生成的数据构建针对特定任务的检索系统。|\[Promptagator: 基于少量示例实现密集检索(opens in a new tab)]\([https://arxiv.org/abs/2209.11755](https://arxiv.org/abs/2209.11755) "Promptagator: 基于少量示例实现密集检索(opens in a new tab)")|2022年9月|
|介绍了Atlas，这是一个经过预训练的检索增强型语言模型，它能够通过极少数的示例学习掌握知识密集任务。|\[Atlas: 借助检索增强型语言模型进行少样本学习(opens in a new tab)]\([https://arxiv.org/abs/2208.03299](https://arxiv.org/abs/2208.03299) "Atlas: 借助检索增强型语言模型进行少样本学习(opens in a new tab)")|2022年8月|
|通过从训练数据中进行智能检索，实现了在多个自然语言生成和理解任务上的性能提升。|\[重新认识训练数据的价值：通过训练数据检索的简单有效方法(opens in a new tab)]\([https://arxiv.org/abs/2203.08773](https://arxiv.org/abs/2203.08773) "重新认识训练数据的价值：通过训练数据检索的简单有效方法(opens in a new tab)")|2022年3月|
|通过在连续的数据存储条目之间建立指针关联，并将这些条目分组成不同的状态，我们近似模拟了数据存储搜索过程。这种方法创造了一个加权有限自动机，在推理时能够在不降低模型预测准确性（困惑度）的情况下，节约高达 83% 的查找最近邻居的计算量。|\[通过自动机增强检索的神经符号语言建模(opens in a new tab)]\([https://arxiv.org/abs/2201.12431](https://arxiv.org/abs/2201.12431) "通过自动机增强检索的神经符号语言建模(opens in a new tab)")|2022 年 1 月|
|通过将自回归语言模型与从大规模文本库中检索的文档块相结合，基于这些文档与前文 Token 的局部相似性，我们实现了模型的显著改进。该策略利用了一个庞大的数据库（2 万亿 Token），大大增强了语言模型的能力。|\[通过从数万亿 Token 中检索来改善语言模型(opens in a new tab)]\([https://arxiv.org/abs/2112.04426](https://arxiv.org/abs/2112.04426) "通过从数万亿 Token 中检索来改善语言模型(opens in a new tab)")|2021 年 12 月|
|我们采用了一种创新的零样本任务处理方法，通过为检索增强生成模型引入严格的负样本和强化训练流程，提升了密集段落检索的效果，用于零样本槽填充任务。|\[用于零样本槽填充的鲁棒检索增强生成(opens in a new tab)]\([https://arxiv.org/abs/2108.13934](https://arxiv.org/abs/2108.13934) "用于零样本槽填充的鲁棒检索增强生成(opens in a new tab)")|2021 年 8 月|
|介绍了 RAG 模型，这是一种结合了预训练的 seq2seq 模型（作为参数记忆）和基于密集向量索引的 Wikipedia（作为非参数记忆）的模型。此模型通过预训练的神经网络检索器访问信息，比较了两种 RAG 设计：一种是在生成过程中始终依赖相同检索的段落，另一种则是每个 Token 都使用不同的段落。|\[用于知识密集型 NLP 任务的检索增强生成(opens in a new tab)]\([https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401) "用于知识密集型 NLP 任务的检索增强生成(opens in a new tab)")|2020 年 5 月|
|展示了一种仅通过密集表示实现信息检索的方法，该方法通过简单的双编码框架从少量问题和文本段落中学习嵌入。这种方法为开放域问答提供了一种高效的密集段落检索方案。|\[用于开放域问答的密集段落检索(opens in a new tab)]\([https://arxiv.org/abs/2004.04906](https://arxiv.org/abs/2004.04906) "用于开放域问答的密集段落检索(opens in a new tab)")|2020 年 4 月|

建立关键信息的自动核查机制，或实施人机协作的审核流程。评估幻觉程度的指标包括事实一致性、引用准确性和自洽性评分。
